{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9Byw8xf_TAx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
        "os.environ[\"CUDNN_DETERMINISTIC\"] = \"1\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uK7ML_kuYTX"
      },
      "outputs": [],
      "source": [
        "import scanpy as sc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from tqdm import tqdm\n",
        "from torch_geometric.utils import softmax\n",
        "from torch_geometric.nn import global_add_pool, global_mean_pool\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XZISa6l0RCI"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def set_seeds(seed):\n",
        "  random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  torch.use_deterministic_algorithms(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OfBGcOTd3pp"
      },
      "outputs": [],
      "source": [
        "adata = sc.read_h5ad(\"covid.h5ad\") # change file name here (cardio.h5ad, icb.h5ad)\n",
        "adata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCrsSwRmweSS"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    df = pd.DataFrame(adata.X.toarray())\n",
        "except:\n",
        "    df = pd.DataFrame(adata.X)\n",
        "df.index = adata.obs.index\n",
        "df[[\"patient\",\"cell_type_annotation\", \"response\"]] = adata.obs[[\"patient\",\"cell_type_annotation\", \"label\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBLJsPifAlH9"
      },
      "outputs": [],
      "source": [
        "def get_data_batch_count(df, all_ct, samples, meta=None, binary=True, attn2=True):\n",
        "  ct_dict = dict({ct: idx for idx, ct in enumerate(all_ct)})\n",
        "  Xs = []\n",
        "  ys = []\n",
        "  batches = []\n",
        "\n",
        "  if meta is not None:\n",
        "    meta = torch.tensor(meta.loc[samples, :].to_numpy(), dtype=torch.float)\n",
        "\n",
        "  for idx, sample in enumerate(samples):\n",
        "    sample_df = df[df[\"patient\"]==sample]\n",
        "    x = sample_df.iloc[:,:df.shape[-1]-3].to_numpy()\n",
        "    y = sample_df.iloc[:,-1].to_numpy()[0]\n",
        "    batch = [(idx * len(all_ct) + ct_dict[ct]) for ct in sample_df[\"cell_type_annotation\"].to_list()]\\\n",
        "            if attn2 else [idx for _ in range(len(sample_df))]\n",
        "    Xs.append(x)\n",
        "    ys.append(y)\n",
        "    batches.append(batch)\n",
        "  Xs = torch.tensor(np.concatenate(Xs), dtype = torch.float)\n",
        "  ys = torch.tensor(ys, dtype = torch.float if binary else torch.long)\n",
        "  batches = torch.tensor(np.concatenate(batches))\n",
        "  return Xs, ys, batches, meta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDegueIMYZHj"
      },
      "outputs": [],
      "source": [
        "class Model(torch.nn.Module):\n",
        "  def __init__(self, n_in, n_out=1, n_in_meta=0, attn1=True, attn2=True, dropout=0.0, use_softmax=True, n_layers_lin=1, n_layers_lin2=0, n_layers_lin_meta=1, n_hid=32, n_hid2=32):\n",
        "    super().__init__()\n",
        "    self.lin = torch.nn.Sequential(\n",
        "        *self.get_lin_layers(n_layers_lin, n_in, n_hid, n_hid, dropout)\n",
        "    )\n",
        "    curr_in = n_in if len(self.lin)==0 else n_hid\n",
        "    self.w_c = torch.nn.Sequential(\n",
        "        torch.nn.Linear(curr_in, 1),\n",
        "        torch.nn.Dropout(dropout)\n",
        "    )\n",
        "    self.n_in1 = curr_in\n",
        "    self.lin2 = torch.nn.Sequential(\n",
        "        *self.get_lin_layers(n_layers_lin2, curr_in, n_hid2, n_hid2, dropout)\n",
        "    )\n",
        "    curr_in = curr_in if len(self.lin2)==0 else n_hid2\n",
        "    self.w_ct = torch.nn.Sequential(\n",
        "        torch.nn.Linear(curr_in, 1),\n",
        "        torch.nn.Dropout(dropout)\n",
        "    )\n",
        "    if n_in_meta > 0:\n",
        "        self.lin_meta = torch.nn.Sequential(\n",
        "            *self.get_lin_layers(n_layers_lin_meta, n_in_meta, curr_in, curr_in, dropout)\n",
        "        )\n",
        "        curr_in += (n_in_meta if n_layers_lin_meta == 0 else curr_in)\n",
        "    self.lin_out = torch.nn.Linear(curr_in, n_out)\n",
        "    self.attn1 = attn1\n",
        "    self.attn2 = attn2\n",
        "    self.use_softmax = use_softmax\n",
        "\n",
        "  def get_lin_layers(self, n_layers, n_in, n_hid, n_out, dropout):\n",
        "    layers = []\n",
        "    for i in range(n_layers):\n",
        "      curr_in = n_in if i == 0 else n_hid\n",
        "      curr_out = n_out if i == n_layers - 1 else n_hid\n",
        "      layers.extend([torch.nn.Linear(curr_in, curr_out), torch.nn.ReLU(), torch.nn.Dropout(dropout)])\n",
        "    return layers\n",
        "\n",
        "  def forward(self, X, batch, ct_size, n_ct, meta=None):\n",
        "    X = self.lin(X)\n",
        "    if self.attn1:\n",
        "        if self.use_softmax:\n",
        "            w_c = softmax(self.w_c(X).squeeze(), batch)\n",
        "        else:\n",
        "            w_c = torch.sigmoid((self.w_c(X)).squeeze())\n",
        "        if self.attn2:\n",
        "            X = global_add_pool(X * w_c.unsqueeze(dim=-1), batch, size=ct_size).reshape(-1, n_ct, self.n_in1)\n",
        "        else:\n",
        "            X = global_add_pool(X * w_c.unsqueeze(dim=-1), batch)\n",
        "    else:\n",
        "        if self.attn2:\n",
        "            X = global_mean_pool(X, batch, size=ct_size).reshape(-1, n_ct, self.n_in1)\n",
        "        else:\n",
        "            X = global_mean_pool(X, batch)\n",
        "    X = self.lin2(X)\n",
        "    if self.attn2:\n",
        "        if self.use_softmax:\n",
        "            w_ct = torch.nn.Softmax(dim=1)(self.w_ct(X))\n",
        "        else:\n",
        "            w_ct = torch.sigmoid(self.w_ct(X))\n",
        "        X = torch.sum(X * w_ct, dim=1)\n",
        "    if meta is not None:\n",
        "        meta = self.lin_meta(meta)\n",
        "        X = torch.cat([X, meta], dim=1)\n",
        "    X = self.lin_out(X)\n",
        "    return X\n",
        "\n",
        "  def decompose_logits(self, X, batch, ct_size, n_ct):\n",
        "    X = self.lin(X)\n",
        "    w_c = softmax(self.w_c(X).squeeze(), batch)\n",
        "    X = global_add_pool(X * w_c.unsqueeze(dim=-1), batch, size=ct_size).reshape(-1, n_ct, self.n_in1)\n",
        "    X = self.lin2(X)\n",
        "    w_ct = torch.nn.Softmax(dim=1)(self.w_ct(X))\n",
        "    X = X @ self.lin_out.weight.T\n",
        "    return (w_ct * X).squeeze(), w_ct.squeeze()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usGBXHrMUBG_"
      },
      "source": [
        "## Cross val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSTz8hPmLCLG"
      },
      "outputs": [],
      "source": [
        "# CHANGE THIS\n",
        "attn1 = True\n",
        "attn2 = True\n",
        "n = 10\n",
        "n_skf = 10\n",
        "n_skf_in = 10\n",
        "meta = None\n",
        "use_meta = False\n",
        "\n",
        "\n",
        "binary = len(set(adata.obs[\"label\"])) == 2\n",
        "n_classes = len(set(adata.obs[\"label\"]))\n",
        "\n",
        "def wrapper_objective(train_samples):\n",
        "    def objective(trial):\n",
        "        n_epochs = trial.suggest_categorical(\"n_epochs\", [100, 500, 1000])\n",
        "        dropout = trial.suggest_categorical(\"dropout\", [0, 0.3, 0.5, 0.7])\n",
        "        weight_decay = trial.suggest_categorical(\"weight_decay\", [1e-4, 1e-3, 1e-2])\n",
        "        n_layers_lin = trial.suggest_categorical(\"n_layers_lin\", [1, 2])\n",
        "        n_hid = trial.suggest_categorical(\"n_hid\", [32, 64, 128])\n",
        "        lr = trial.suggest_categorical(\"lr\", [1e-3, 5e-3])\n",
        "        n_layers_lin_meta = trial.suggest_categorical(\"n_layers_lin_meta\", [0, 1, 2]) if use_meta else 1\n",
        "\n",
        "        skf = StratifiedKFold(n_skf_in, shuffle=True, random_state=0)\n",
        "        preds_ = []\n",
        "        truths_ = []\n",
        "\n",
        "        for train_idx, val_idx in skf.split(train_samples, train_samples[\"label\"]):\n",
        "            train_samples_in = train_samples.iloc[train_idx, :][\"patient\"].to_list()\n",
        "            val_samples = train_samples.iloc[val_idx, :][\"patient\"].to_list()\n",
        "\n",
        "            X_train, y_train, batch_train, meta_train = get_data_batch_count(df, all_ct, train_samples_in, binary=binary, meta=meta if use_meta else None, attn2=attn2)\n",
        "            X_val,y_val, batch_val, meta_val = get_data_batch_count(df, all_ct, val_samples, binary=binary, meta=meta if use_meta else None, attn2=attn2)\n",
        "\n",
        "            X_train, y_train, batch_train, meta_train = X_train.to(device), y_train.to(device), batch_train.to(device), meta_train.to(device) if meta_train is not None else meta_train\n",
        "            X_val, y_val, batch_val, meta_val = X_val.to(device), y_val.to(device), batch_val.to(device), meta_val.to(device) if meta_val is not None else meta_val\n",
        "\n",
        "            set_seeds(0)\n",
        "            model = Model(X_train.shape[-1], n_out=1 if binary else n_classes, n_in_meta=0 if not use_meta else meta_train.shape[-1], \\\n",
        "                        attn1=attn1, attn2=attn2, use_softmax=True, dropout=dropout, n_layers_lin=n_layers_lin, n_layers_lin2=0, \\\n",
        "                        n_layers_lin_meta=n_layers_lin_meta, n_hid=n_hid, n_hid2=0).to(device)\n",
        "            opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "            loss_fn = torch.nn.BCEWithLogitsLoss() if binary else torch.nn.CrossEntropyLoss()\n",
        "\n",
        "            for epoch in range(n_epochs):\n",
        "                model.train()\n",
        "                opt.zero_grad()\n",
        "                pred = model(X_train, batch_train, len(all_ct)*len(y_train), len(all_ct), meta=meta_train)\n",
        "                loss = loss_fn(pred.squeeze(), y_train.squeeze())\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                pred = model(X_val, batch_val, len(all_ct)*len(y_val), len(all_ct), meta=meta_val)\n",
        "                pred = torch.sigmoid(pred.squeeze()) if binary else torch.softmax(pred.squeeze(), -1)\n",
        "                preds_.extend(pred.detach().cpu().numpy())\n",
        "                truths_.extend(y_val.squeeze().cpu().numpy())\n",
        "\n",
        "        return roc_auc_score(np.stack(truths_), np.stack(preds_), multi_class=\"ovo\")\n",
        "    return objective\n",
        "\n",
        "\n",
        "import optuna as optuna\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "all_ct = adata.obs[\"cell_type_annotation\"].unique()\n",
        "samples = adata.obs[[\"patient\", \"label\"]].drop_duplicates()\n",
        "\n",
        "def run():\n",
        "    aucs = []\n",
        "    for i in tqdm(range(n)):\n",
        "        skf = StratifiedKFold(n_skf, shuffle=True, random_state=i)\n",
        "        preds_ = []\n",
        "        truths_ = []\n",
        "        for train_idx, test_idx in skf.split(samples, samples[\"label\"]):\n",
        "\n",
        "            train_samples = samples.iloc[train_idx, :]\n",
        "            test_samples = samples.iloc[test_idx, :][\"patient\"].to_list()\n",
        "\n",
        "            X_test, y_test, batch_test, meta_test  = get_data_batch_count(df, all_ct, test_samples, binary=binary, meta=meta if use_meta else None, attn2=attn2)\n",
        "            X_test, y_test, batch_test, meta_test = X_test.to(device), y_test.to(device), batch_test.to(device), meta_test.to(device) if meta_test is not None else meta_test\n",
        "\n",
        "            sampler = TPESampler(seed=0)\n",
        "            study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
        "            study.optimize(wrapper_objective(train_samples), n_trials=30)\n",
        "            best_params = study.best_params\n",
        "\n",
        "            X_train, y_train, batch_train, meta_train = get_data_batch_count(df, all_ct, train_samples[\"patient\"].to_list(), binary=binary, meta=meta if use_meta else None, attn2=attn2)\n",
        "            X_train, y_train, batch_train, meta_train = X_train.to(device), y_train.to(device), batch_train.to(device), meta_train.to(device) if meta_train is not None else meta_train\n",
        "\n",
        "            set_seeds(i)\n",
        "            model = Model(X_train.shape[-1], n_out=1 if binary else n_classes, n_in_meta=0 if not use_meta else meta_train.shape[-1], \\\n",
        "                        attn1=attn1, attn2=attn2, use_softmax=True, dropout=best_params[\"dropout\"], \\\n",
        "                        n_layers_lin=best_params[\"n_layers_lin\"], n_layers_lin2=0, \\\n",
        "                        n_layers_lin_meta=1 if not use_meta else best_params[\"n_layers_lin_meta\"], n_hid=best_params[\"n_hid\"], n_hid2=0).to(device)\n",
        "            opt = torch.optim.Adam(model.parameters(), lr=best_params[\"lr\"], weight_decay=best_params[\"weight_decay\"])\n",
        "            loss_fn = torch.nn.BCEWithLogitsLoss() if binary else torch.nn.CrossEntropyLoss()\n",
        "\n",
        "            for epoch in range(best_params[\"n_epochs\"]):\n",
        "                model.train()\n",
        "                opt.zero_grad()\n",
        "                pred = model(X_train, batch_train, len(all_ct)*len(y_train), len(all_ct), meta=meta_train)\n",
        "                loss = loss_fn(pred.squeeze(), y_train.squeeze())\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                pred = model(X_test, batch_test, len(all_ct)*len(y_test), len(all_ct), meta=meta_test)\n",
        "                pred = torch.sigmoid(pred.squeeze()) if binary else torch.softmax(pred.squeeze(), -1)\n",
        "                preds_.extend(pred.detach().cpu().numpy())\n",
        "                truths_.extend(y_test.squeeze().cpu().numpy())\n",
        "        aucs.append(roc_auc_score(np.stack(truths_), np.stack(preds_), multi_class=\"ovo\"))\n",
        "    return np.mean(aucs), np.std(aucs)\n",
        "mean_auc, std_auc = run()\n",
        "print(f\"AUC: {mean_auc} +/- {std_auc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD-zA0jLT9GM"
      },
      "source": [
        "## Vary train size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_KL9fteJv9F"
      },
      "outputs": [],
      "source": [
        "# CHANGE THIS\n",
        "train_sizes = [0.25, 0.5, 0.75]\n",
        "n = 100\n",
        "n_skf_in = 4\n",
        "attn1 = True\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "binary = len(set(adata.obs[\"label\"])) == 2\n",
        "n_classes = len(set(adata.obs[\"label\"]))\n",
        "\n",
        "def objective(trial):\n",
        "    n_epochs = trial.suggest_categorical(\"n_epochs\", [100, 500, 1000])\n",
        "    dropout = trial.suggest_categorical(\"dropout\", [0, 0.3, 0.5, 0.7])\n",
        "    weight_decay = trial.suggest_categorical(\"weight_decay\", [1e-4, 1e-3, 1e-2])\n",
        "    n_layers_lin = trial.suggest_categorical(\"n_layers_lin\", [1, 2])\n",
        "    n_hid = trial.suggest_categorical(\"n_hid\", [32, 64, 128])\n",
        "    lr = trial.suggest_categorical(\"lr\", [1e-3, 5e-3])\n",
        "\n",
        "    skf = StratifiedKFold(n_skf_in, shuffle=True, random_state=0)\n",
        "    preds_ = []\n",
        "    truths_ = []\n",
        "\n",
        "    for train_idx, val_idx in skf.split(train_samples, train_samples[\"label\"]):\n",
        "        train_samples_in = train_samples.iloc[train_idx, :][\"patient\"].to_list()\n",
        "        val_samples = train_samples.iloc[val_idx, :][\"patient\"].to_list()\n",
        "\n",
        "        X_train, y_train, batch_train, _ = get_data_batch_count(df, all_ct, train_samples_in, binary=binary)\n",
        "        X_val,y_val, batch_val, _ = get_data_batch_count(df, all_ct, val_samples, binary=binary)\n",
        "\n",
        "        X_train, y_train, batch_train = X_train.to(device), y_train.to(device), batch_train.to(device)\n",
        "        X_val, y_val, batch_val = X_val.to(device), y_val.to(device), batch_val.to(device)\n",
        "\n",
        "        set_seeds(0)\n",
        "        model = Model(X_train.shape[-1], n_out=1 if binary else n_classes, \\\n",
        "                      attn1=attn1, attn2=True, use_softmax=True, dropout=dropout, \\\n",
        "                    n_layers_lin=n_layers_lin, n_layers_lin2=0, n_hid=n_hid, n_hid2=0).to(device)\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        loss_fn = torch.nn.BCEWithLogitsLoss() if binary else torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            model.train()\n",
        "            opt.zero_grad()\n",
        "            pred = model(X_train, batch_train, len(all_ct)*len(y_train), len(all_ct))\n",
        "            loss = loss_fn(pred.squeeze(), y_train.squeeze())\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            pred = model(X_val, batch_val, len(all_ct)*len(y_val), len(all_ct))\n",
        "            pred = torch.sigmoid(pred.squeeze()) if binary else torch.softmax(pred.squeeze(), -1)\n",
        "            preds_.extend(pred.detach().cpu().numpy())\n",
        "            truths_.extend(y_val.squeeze().cpu().numpy())\n",
        "\n",
        "    return roc_auc_score(np.stack(truths_), np.stack(preds_), multi_class=\"ovo\")\n",
        "\n",
        "\n",
        "import optuna as optuna\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "all_ct = adata.obs[\"cell_type_annotation\"].unique()\n",
        "samples = adata.obs[[\"patient\", \"label\"]].drop_duplicates()\n",
        "\n",
        "for train_size in train_sizes:\n",
        "    aucs = []\n",
        "    for i in tqdm(range(n)):\n",
        "        train_idx, test_idx = train_test_split(range(len(samples)), stratify=samples[\"label\"].to_list(), train_size=train_size, random_state=i)\n",
        "        train_samples = samples.iloc[train_idx, :]\n",
        "        test_samples = samples.iloc[test_idx, :][\"patient\"].to_list()\n",
        "\n",
        "        X_test, y_test, batch_test, _  = get_data_batch_count(df, all_ct, test_samples, binary=binary)\n",
        "        X_test, y_test, batch_test = X_test.to(device), y_test.to(device), batch_test.to(device)\n",
        "\n",
        "        sampler = TPESampler(seed=0)\n",
        "        study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
        "        study.optimize(objective, n_trials=30)\n",
        "        best_params = study.best_params\n",
        "\n",
        "        X_train, y_train, batch_train, _  = get_data_batch_count(df, all_ct, train_samples[\"patient\"].to_list(), binary=binary)\n",
        "        X_train, y_train, batch_train = X_train.to(device), y_train.to(device), batch_train.to(device)\n",
        "        set_seeds(i)\n",
        "        model = Model(X_train.shape[-1], n_out=1 if binary else n_classes, \\\n",
        "                      attn1=attn1, attn2=True, use_softmax=True, dropout=best_params[\"dropout\"], \\\n",
        "                      n_layers_lin=best_params[\"n_layers_lin\"], n_layers_lin2=0, n_hid=best_params[\"n_hid\"], n_hid2=0).to(device)\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=best_params[\"lr\"], weight_decay=best_params[\"weight_decay\"])\n",
        "        loss_fn = torch.nn.BCEWithLogitsLoss() if binary else torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(best_params[\"n_epochs\"]):\n",
        "            model.train()\n",
        "            opt.zero_grad()\n",
        "            pred = model(X_train, batch_train, len(all_ct)*len(y_train), len(all_ct))\n",
        "            loss = loss_fn(pred.squeeze(), y_train.squeeze())\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            pred = model(X_test, batch_test, len(all_ct)*len(y_test), len(all_ct))\n",
        "            pred = torch.sigmoid(pred.squeeze()) if binary else torch.softmax(pred.squeeze(), -1)\n",
        "        auc = roc_auc_score(y_test.squeeze().cpu().numpy(), pred.detach().cpu().numpy(), multi_class=\"ovo\")\n",
        "        aucs.append(auc)\n",
        "    print(\"Train size\", train_size, np.mean(aucs), \"+/-\", np.std(aucs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rw5coMCkmXiz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akKdA6XrWYoJ"
      },
      "source": [
        "## Vary cell count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7sN5DKmWbWS"
      },
      "outputs": [],
      "source": [
        "# CHANGE THIS\n",
        "cell_props = [0.25, 0.5, 0.75]\n",
        "n = 10\n",
        "n_skf = 10\n",
        "n_skf_in = 10\n",
        "attn1 = True\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "binary = len(set(adata.obs[\"label\"])) == 2\n",
        "n_classes = len(set(adata.obs[\"label\"]))\n",
        "def objective(trial):\n",
        "    n_epochs = trial.suggest_categorical(\"n_epochs\", [100, 500, 1000])\n",
        "    dropout = trial.suggest_categorical(\"dropout\", [0, 0.3, 0.5, 0.7])\n",
        "    weight_decay = trial.suggest_categorical(\"weight_decay\", [1e-4, 1e-3, 1e-2])\n",
        "    n_layers_lin = trial.suggest_categorical(\"n_layers_lin\", [1, 2])\n",
        "    n_hid = trial.suggest_categorical(\"n_hid\", [32, 64, 128])\n",
        "    lr = trial.suggest_categorical(\"lr\", [1e-3, 5e-3])\n",
        "\n",
        "    skf = StratifiedKFold(n_skf_in, shuffle=True, random_state=0)\n",
        "    preds_ = []\n",
        "    truths_ = []\n",
        "\n",
        "    for train_idx, val_idx in skf.split(train_samples, train_samples[\"label\"]):\n",
        "        train_samples_in = train_samples.iloc[train_idx, :][\"patient\"].to_list()\n",
        "        val_samples = train_samples.iloc[val_idx, :][\"patient\"].to_list()\n",
        "\n",
        "        X_train, y_train, batch_train, _ = get_data_batch_count(df_subsampled, all_ct, train_samples_in, binary=binary)\n",
        "        X_val,y_val, batch_val, _ = get_data_batch_count(df_subsampled, all_ct, val_samples, binary=binary)\n",
        "\n",
        "        X_train, y_train, batch_train = X_train.to(device), y_train.to(device), batch_train.to(device)\n",
        "        X_val, y_val, batch_val = X_val.to(device), y_val.to(device), batch_val.to(device)\n",
        "\n",
        "        set_seeds(0)\n",
        "        model = Model(X_train.shape[-1], n_out=1 if binary else n_classes, \\\n",
        "                      attn1=attn1, attn2=True, use_softmax=True, dropout=dropout, \\\n",
        "                    n_layers_lin=n_layers_lin, n_layers_lin2=0, n_hid=n_hid, n_hid2=0).to(device)\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        loss_fn = torch.nn.BCEWithLogitsLoss() if binary else torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            model.train()\n",
        "            opt.zero_grad()\n",
        "            pred = model(X_train, batch_train, len(all_ct)*len(y_train), len(all_ct))\n",
        "            loss = loss_fn(pred.squeeze(), y_train.squeeze())\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            pred = model(X_val, batch_val, len(all_ct)*len(y_val), len(all_ct))\n",
        "            pred = torch.sigmoid(pred.squeeze()) if binary else torch.softmax(pred.squeeze(), -1)\n",
        "            preds_.extend(pred.detach().cpu().numpy())\n",
        "            truths_.extend(y_val.squeeze().cpu().numpy())\n",
        "\n",
        "    return roc_auc_score(np.stack(truths_), np.stack(preds_), multi_class=\"ovo\")\n",
        "\n",
        "\n",
        "import optuna as optuna\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "all_ct = adata.obs[\"cell_type_annotation\"].unique()\n",
        "samples = adata.obs[[\"patient\", \"label\"]].drop_duplicates()\n",
        "\n",
        "for cell_prop in cell_props:\n",
        "    aucs = []\n",
        "    for i in tqdm(range(n)):\n",
        "        skf = StratifiedKFold(n_skf, shuffle=True, random_state=i)\n",
        "        preds_ = []\n",
        "        truths_ = []\n",
        "        for train_idx, test_idx in skf.split(samples, samples[\"label\"]):\n",
        "\n",
        "            df_subsampled = df.groupby('patient', observed=False).apply(lambda x: x.sample(frac=cell_prop, random_state=i), include_groups=True).reset_index(drop=True)\n",
        "            train_samples = samples.iloc[train_idx, :]\n",
        "            test_samples = samples.iloc[test_idx, :][\"patient\"].to_list()\n",
        "\n",
        "            X_test, y_test, batch_test, _  = get_data_batch_count(df_subsampled, all_ct, test_samples, binary=binary)\n",
        "            X_test, y_test, batch_test = X_test.to(device), y_test.to(device), batch_test.to(device)\n",
        "\n",
        "\n",
        "            sampler = TPESampler(seed=0)\n",
        "            study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
        "            study.optimize(objective, n_trials=30)\n",
        "            best_params = study.best_params\n",
        "\n",
        "\n",
        "            X_train, y_train, batch_train, _  = get_data_batch_count(df_subsampled, all_ct, train_samples[\"patient\"].to_list(), binary=binary)\n",
        "            X_train, y_train, batch_train = X_train.to(device), y_train.to(device), batch_train.to(device)\n",
        "            set_seeds(i)\n",
        "            model = Model(X_train.shape[-1], n_out=1 if binary else n_classes, \\\n",
        "                        attn1=attn1, attn2=True, use_softmax=True, dropout=best_params[\"dropout\"], \\\n",
        "                        n_layers_lin=best_params[\"n_layers_lin\"], n_layers_lin2=0, n_hid=best_params[\"n_hid\"], n_hid2=0).to(device)\n",
        "            opt = torch.optim.Adam(model.parameters(), lr=best_params[\"lr\"], weight_decay=best_params[\"weight_decay\"])\n",
        "            loss_fn = torch.nn.BCEWithLogitsLoss() if binary else torch.nn.CrossEntropyLoss()\n",
        "\n",
        "            for epoch in range(best_params[\"n_epochs\"]):\n",
        "                model.train()\n",
        "                opt.zero_grad()\n",
        "                pred = model(X_train, batch_train, len(all_ct)*len(y_train), len(all_ct))\n",
        "                loss = loss_fn(pred.squeeze(), y_train.squeeze())\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                pred = model(X_test, batch_test, len(all_ct)*len(y_test), len(all_ct))\n",
        "                pred = torch.sigmoid(pred.squeeze()) if binary else torch.softmax(pred.squeeze(), -1)\n",
        "                preds_.extend(pred.detach().cpu().numpy())\n",
        "                truths_.extend(y_test.squeeze().cpu().numpy())\n",
        "        aucs.append(roc_auc_score(np.stack(truths_), np.stack(preds_), multi_class=\"ovo\"))\n",
        "    print(\"Cell prop\", cell_prop, np.mean(aucs), \"+/-\", np.std(aucs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFuLdIXxUM-B"
      },
      "source": [
        "## Vary cell type annot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4k_b8NwvUQHs"
      },
      "outputs": [],
      "source": [
        "# CHANGE THIS\n",
        "cell_props = [0.25, 0.5]\n",
        "n = 10\n",
        "n_skf = 10\n",
        "n_skf_in = 10\n",
        "attn1 = True\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "def reassign_cell_types(df, prop, all_ct, seed=0):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    df = df.copy()\n",
        "    patients = df['patient'].unique()\n",
        "\n",
        "    for patient in patients:\n",
        "        patient_data = df[df['patient'] == patient]\n",
        "        num_to_select = int(len(patient_data) * prop)\n",
        "\n",
        "        selected_indices = np.random.choice(patient_data.index, num_to_select, replace=False)\n",
        "\n",
        "        new_annotations = np.random.choice(all_ct, num_to_select, replace=True)\n",
        "\n",
        "        df.loc[selected_indices, 'cell_type_annotation'] = new_annotations\n",
        "\n",
        "    return df\n",
        "\n",
        "binary = len(set(adata.obs[\"label\"])) == 2\n",
        "n_classes = len(set(adata.obs[\"label\"]))\n",
        "def objective(trial):\n",
        "    n_epochs = trial.suggest_categorical(\"n_epochs\", [100, 500, 1000])\n",
        "    dropout = trial.suggest_categorical(\"dropout\", [0, 0.3, 0.5, 0.7])\n",
        "    weight_decay = trial.suggest_categorical(\"weight_decay\", [1e-4, 1e-3, 1e-2])\n",
        "    n_layers_lin = trial.suggest_categorical(\"n_layers_lin\", [1, 2])\n",
        "    n_hid = trial.suggest_categorical(\"n_hid\", [32, 64, 128])\n",
        "    lr = trial.suggest_categorical(\"lr\", [1e-3, 5e-3])\n",
        "\n",
        "    skf = StratifiedKFold(n_skf_in, shuffle=True, random_state=0)\n",
        "    preds_ = []\n",
        "    truths_ = []\n",
        "\n",
        "    for train_idx, val_idx in skf.split(train_samples, train_samples[\"label\"]):\n",
        "        train_samples_in = train_samples.iloc[train_idx, :][\"patient\"].to_list()\n",
        "        val_samples = train_samples.iloc[val_idx, :][\"patient\"].to_list()\n",
        "\n",
        "        X_train, y_train, batch_train, _ = get_data_batch_count(df_subsampled, all_ct, train_samples_in, binary=binary)\n",
        "        X_val,y_val, batch_val, _ = get_data_batch_count(df_subsampled, all_ct, val_samples, binary=binary)\n",
        "\n",
        "        X_train, y_train, batch_train = X_train.to(device), y_train.to(device), batch_train.to(device)\n",
        "        X_val, y_val, batch_val = X_val.to(device), y_val.to(device), batch_val.to(device)\n",
        "\n",
        "        set_seeds(0)\n",
        "        model = Model(X_train.shape[-1], n_out=1 if binary else n_classes, \\\n",
        "                      attn1=attn1, attn2=True, use_softmax=True, dropout=dropout, \\\n",
        "                    n_layers_lin=n_layers_lin, n_layers_lin2=0, n_hid=n_hid, n_hid2=0).to(device)\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        loss_fn = torch.nn.BCEWithLogitsLoss() if binary else torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            model.train()\n",
        "            opt.zero_grad()\n",
        "            pred = model(X_train, batch_train, len(all_ct)*len(y_train), len(all_ct))\n",
        "            loss = loss_fn(pred.squeeze(), y_train.squeeze())\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            pred = model(X_val, batch_val, len(all_ct)*len(y_val), len(all_ct))\n",
        "            pred = torch.sigmoid(pred.squeeze()) if binary else torch.softmax(pred.squeeze(), -1)\n",
        "            preds_.extend(pred.detach().cpu().numpy())\n",
        "            truths_.extend(y_val.squeeze().cpu().numpy())\n",
        "\n",
        "    return roc_auc_score(np.stack(truths_), np.stack(preds_), multi_class=\"ovo\")\n",
        "\n",
        "\n",
        "import optuna as optuna\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "all_ct = adata.obs[\"cell_type_annotation\"].unique()\n",
        "samples = adata.obs[[\"patient\", \"label\"]].drop_duplicates()\n",
        "\n",
        "for cell_prop in cell_props:\n",
        "    aucs = []\n",
        "    for i in tqdm(range(n)):\n",
        "        skf = StratifiedKFold(n_skf, shuffle=True, random_state=i)\n",
        "        preds_ = []\n",
        "        truths_ = []\n",
        "        for train_idx, test_idx in skf.split(samples, samples[\"label\"]):\n",
        "\n",
        "            df_subsampled = reassign_cell_types(df, cell_prop, all_ct, seed=i)\n",
        "            train_samples = samples.iloc[train_idx, :]\n",
        "            test_samples = samples.iloc[test_idx, :][\"patient\"].to_list()\n",
        "\n",
        "            X_test, y_test, batch_test, _  = get_data_batch_count(df_subsampled, all_ct, test_samples, binary=binary)\n",
        "            X_test, y_test, batch_test = X_test.to(device), y_test.to(device), batch_test.to(device)\n",
        "\n",
        "\n",
        "            sampler = TPESampler(seed=0)\n",
        "            study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
        "            study.optimize(objective, n_trials=30)\n",
        "            best_params = study.best_params\n",
        "\n",
        "\n",
        "            X_train, y_train, batch_train, _  = get_data_batch_count(df_subsampled, all_ct, train_samples[\"patient\"].to_list(), binary=binary)\n",
        "            X_train, y_train, batch_train = X_train.to(device), y_train.to(device), batch_train.to(device)\n",
        "            set_seeds(i)\n",
        "            model = Model(X_train.shape[-1], n_out=1 if binary else n_classes, \\\n",
        "                        attn1=attn1, attn2=True, use_softmax=True, dropout=best_params[\"dropout\"], \\\n",
        "                        n_layers_lin=best_params[\"n_layers_lin\"], n_layers_lin2=0, n_hid=best_params[\"n_hid\"], n_hid2=0).to(device)\n",
        "            opt = torch.optim.Adam(model.parameters(), lr=best_params[\"lr\"], weight_decay=best_params[\"weight_decay\"])\n",
        "            loss_fn = torch.nn.BCEWithLogitsLoss() if binary else torch.nn.CrossEntropyLoss()\n",
        "\n",
        "            for epoch in range(best_params[\"n_epochs\"]):\n",
        "                model.train()\n",
        "                opt.zero_grad()\n",
        "                pred = model(X_train, batch_train, len(all_ct)*len(y_train), len(all_ct))\n",
        "                loss = loss_fn(pred.squeeze(), y_train.squeeze())\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                pred = model(X_test, batch_test, len(all_ct)*len(y_test), len(all_ct))\n",
        "                pred = torch.sigmoid(pred.squeeze()) if binary else torch.softmax(pred.squeeze(), -1)\n",
        "                preds_.extend(pred.detach().cpu().numpy())\n",
        "                truths_.extend(y_test.squeeze().cpu().numpy())\n",
        "        aucs.append(roc_auc_score(np.stack(truths_), np.stack(preds_), multi_class=\"ovo\"))\n",
        "    print(\"Cell prop\", cell_prop, np.mean(aucs), \"+/-\", np.std(aucs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgLd-uYXxj6y"
      },
      "source": [
        "## Perm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CHANGE THIS\n",
        "n_perm = 100\n",
        "dropout = 0.5\n",
        "lr = 1e-3\n",
        "weight_decay = 1e-3\n",
        "n_epochs = 1000\n",
        "n_layers_lin = 1\n",
        "n_hid = 32\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggFUooESKDO6"
      },
      "outputs": [],
      "source": [
        "def get_data_batch_count_perm(tmp, all_ct, samples, meta=None, perm_annot=False, seed=None, binary=True):\n",
        "    ct_dict = dict({ct: idx for idx, ct in enumerate(all_ct)})\n",
        "    Xs = []\n",
        "    batches = []\n",
        "\n",
        "    if meta is not None:\n",
        "        meta = torch.tensor(meta.loc[samples[\"patient\"], :].to_numpy(), dtype=torch.float)\n",
        "\n",
        "    for idx, sample in enumerate(samples[\"patient\"].to_list()):\n",
        "        sample_df = tmp[tmp[\"patient\"]==sample]\n",
        "        x = sample_df.iloc[:,:df.shape[-1]-3].to_numpy()\n",
        "        batch = [(idx * len(all_ct) + ct_dict[ct]) for ct in sample_df[\"cell_type_annotation\"].to_list()]\n",
        "        if perm_annot:\n",
        "            set_seeds(seed)\n",
        "            batch = np.random.permutation(batch)\n",
        "        Xs.append(x)\n",
        "        batches.append(batch)\n",
        "    Xs = torch.tensor(np.concatenate(Xs), dtype = torch.float)\n",
        "    batches = torch.tensor(np.concatenate(batches))\n",
        "    ys = torch.tensor(samples[\"label\"].to_list(), dtype = torch.float if binary else torch.long)\n",
        "    return Xs, ys, batches, meta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jQEIZH2xvWU"
      },
      "outputs": [],
      "source": [
        "\n",
        "n_classes = len(set(adata.obs[\"label\"]))\n",
        "binary = n_classes == 2\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "all_ct = adata.obs[\"cell_type_annotation\"].unique()\n",
        "\n",
        "def train(perm=False, seed=None, reduce=True, n_skf=5):\n",
        "    samples = adata.obs[[\"patient\", \"label\"]].drop_duplicates()\n",
        "    if perm:\n",
        "        samples[\"label\"] = samples[\"label\"].sample(len(samples), random_state=seed).to_list()\n",
        "    skf = StratifiedKFold(n_skf, shuffle=True, random_state=0)\n",
        "    ct_logits = []\n",
        "    truths = []\n",
        "    for train_idx, test_idx in skf.split(samples, samples[\"label\"]):\n",
        "\n",
        "        train_samples = samples.iloc[train_idx, :]\n",
        "        test_samples = samples.iloc[test_idx, :]\n",
        "\n",
        "        X_test, y_test, batch_test, _ = get_data_batch_count_perm(df, all_ct, test_samples, binary=binary)\n",
        "        X_test, y_test, batch_test = X_test.to(device), y_test.to(device), batch_test.to(device)\n",
        "\n",
        "        X_train, y_train, batch_train, _  = get_data_batch_count_perm(df, all_ct, train_samples, binary=binary)\n",
        "        X_train, y_train, batch_train = X_train.to(device), y_train.to(device), batch_train.to(device)\n",
        "        set_seeds(0)\n",
        "        model = Model(X_train.shape[-1], n_out=1 if binary else n_classes, attn1=True, attn2=True, use_softmax=True, dropout=dropout, \\\n",
        "                      n_layers_lin=n_layers_lin, n_layers_lin2=0, n_hid=n_hid, n_hid2=0).to(device)\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        loss_fn = torch.nn.BCEWithLogitsLoss() if binary else torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            model.train()\n",
        "            opt.zero_grad()\n",
        "            pred = model(X_train, batch_train, len(all_ct)*len(y_train), len(all_ct))\n",
        "            loss = loss_fn(pred.squeeze(), y_train.squeeze())\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            ct_logit, _ = model.decompose_logits(X_test, batch_test, len(all_ct)*len(y_test), len(all_ct))\n",
        "            ct_logits.extend(ct_logit)\n",
        "            truths.extend(y_test.long().squeeze().cpu().numpy())\n",
        "    ct_logits = torch.stack(ct_logits).cpu().numpy()\n",
        "    truths = np.array(truths)\n",
        "    if binary:\n",
        "        tmp = pd.DataFrame(ct_logits, columns=all_ct)\n",
        "        tmp[\"label\"] = truths\n",
        "        tmp = tmp.groupby(\"label\").mean().loc[[0,1],:].to_numpy()\n",
        "        if reduce:\n",
        "            tmp = tmp[1] - tmp[0]\n",
        "    else:\n",
        "        tmp = []\n",
        "        for i in range(n_classes):\n",
        "            curr_class_logits = ct_logits[:, :, i]\n",
        "            other_class_logits = ct_logits[:, :, [c for c in range(n_classes) if c != i]].mean(-1)\n",
        "            diff = curr_class_logits - other_class_logits\n",
        "            tmp.append(np.mean(diff[truths == i], 0))\n",
        "        tmp = pd.DataFrame(tmp, columns=all_ct).to_numpy()\n",
        "        if reduce:\n",
        "            tmp = tmp.sum(0)\n",
        "    return tmp, ct_logits, truths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjDpGxb11QDJ"
      },
      "outputs": [],
      "source": [
        "orig = train(False, reduce=False)\n",
        "orig = pd.DataFrame(orig, columns=all_ct, index=[\"normal\", \"covid\"])\n",
        "(orig.iloc[1,:] - orig.iloc[0, :]).sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUwPZFWD3QSv"
      },
      "outputs": [],
      "source": [
        "perms = []\n",
        "for i in tqdm(range(n_perm)):\n",
        "  perms.append(train(True, i))\n",
        "perms = np.stack(perms)\n",
        "truth = train(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbHYVFx21f79"
      },
      "outputs": [],
      "source": [
        "p_vals = (truth[None, :] < perms).sum(0) / n_perm\n",
        "p_vals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Vg7F_1J1hUH"
      },
      "outputs": [],
      "source": [
        "from statsmodels.stats.multitest import multipletests\n",
        "\n",
        "_, p_vals_corrected, _, _ = multipletests(p_vals, alpha=0.05, method='fdr_bh')\n",
        "p_vals_corrected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ct0Ublj1rTr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "usGBXHrMUBG_",
        "tD-zA0jLT9GM",
        "akKdA6XrWYoJ",
        "KFuLdIXxUM-B",
        "fgLd-uYXxj6y"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
